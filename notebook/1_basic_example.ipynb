{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "from aitools_autogen.config import llm_config_llama2\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config_list_openai = [\n",
    "    {\n",
    "        'base_url': 'http://aitools.cs.vt.edu:7860/openai/v1',\n",
    "        'api_key': 'aitools',\n",
    "        'model': 'gpt-4-turbo-preview',\n",
    "    },\n",
    "        {\n",
    "        'base_url': 'http://aitools.cs.vt.edu:7860/openai/v1',\n",
    "        'api_key': 'aitools',\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "llm_config_openai = {\n",
    "    \"timeout\": 300,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list_openai,\n",
    "    \"temperature\": 0.1,\n",
    "    \"allow_format_str_template\": True\n",
    "}\n",
    "\n",
    "# Define an assistant coder using the LLM\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config_openai,\n",
    "    system_message=\"\"\"\n",
    "    You are a professional Python coder, known for your insightful and engaging solutions with explanations to coding problems.\n",
    "    You produce Python code in response to coding problems.  You do not comment on feedback, you respond ONLY with code.\n",
    "    You should improve the quality of the code based on the feedback from the user.\n",
    "    When the user indicates your code is acceptable, send a TERMINATE message.\n",
    "    \"\"\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1a5cad4379f63e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the user proxy, the driver of the conversation who has ability to execute code.\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    max_consecutive_auto_reply=6, # stop infinite chat loops\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 1,\n",
    "        \"work_dir\": \"tasks\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    llm_config=llm_config_llama2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b53c56b74dfbd7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example Task\n",
    "task = f\"\"\"\n",
    "Write a function in Python to sort an array of integers using quicksort.\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68acd8c39dc60b04",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = user_proxy.initiate_chat(recipient=coder, message=task, max_turns=2, summary_method=\"last_msg\")\n",
    "print(res)\n",
    "print(\"---\")\n",
    "print(user_proxy.last_message(coder))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c20279a753870d80",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ffc8c04fc938591",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "16d431a027858c1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
